{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e302f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c889b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "values=iris.data\n",
    "target=iris.target\n",
    "\n",
    "ss=StandardScaler()\n",
    "values=ss.fit_transform(values)\n",
    "\n",
    "values_train,values_test,target_train,target_test=train_test_split(values,target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e54ab427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41834317\n",
      "Iteration 2, loss = 1.15383106\n",
      "Iteration 3, loss = 1.09908184\n",
      "Iteration 4, loss = 1.13276448\n",
      "Iteration 5, loss = 1.14207537\n",
      "Iteration 6, loss = 1.11483085\n",
      "Iteration 7, loss = 1.08456851\n",
      "Iteration 8, loss = 1.06927626\n",
      "Iteration 9, loss = 1.06288267\n",
      "Iteration 10, loss = 1.05403892\n",
      "Iteration 11, loss = 1.03883567\n",
      "Iteration 12, loss = 1.01864550\n",
      "Iteration 13, loss = 0.99428593\n",
      "Iteration 14, loss = 0.96447351\n",
      "Iteration 15, loss = 0.92754738\n",
      "Iteration 16, loss = 0.88323446\n",
      "Iteration 17, loss = 0.83311123\n",
      "Iteration 18, loss = 0.78000023\n",
      "Iteration 19, loss = 0.72705571\n",
      "Iteration 20, loss = 0.67707946\n",
      "Iteration 21, loss = 0.63211868\n",
      "Iteration 22, loss = 0.59328197\n",
      "Iteration 23, loss = 0.56078588\n",
      "Iteration 24, loss = 0.53418419\n",
      "Iteration 25, loss = 0.51266971\n",
      "Iteration 26, loss = 0.49533878\n",
      "Iteration 27, loss = 0.48135066\n",
      "Iteration 28, loss = 0.46998515\n",
      "Iteration 29, loss = 0.46064613\n",
      "Iteration 30, loss = 0.45285038\n",
      "Iteration 31, loss = 0.44621378\n",
      "Iteration 32, loss = 0.44043504\n",
      "Iteration 33, loss = 0.43527845\n",
      "Iteration 34, loss = 0.43055876\n",
      "Iteration 35, loss = 0.42612967\n",
      "Iteration 36, loss = 0.42187536\n",
      "Iteration 37, loss = 0.41770412\n",
      "Iteration 38, loss = 0.41354338\n",
      "Iteration 39, loss = 0.40933567\n",
      "Iteration 40, loss = 0.40503487\n",
      "Iteration 41, loss = 0.40060259\n",
      "Iteration 42, loss = 0.39600481\n",
      "Iteration 43, loss = 0.39120916\n",
      "Iteration 44, loss = 0.38618361\n",
      "Iteration 45, loss = 0.38089662\n",
      "Iteration 46, loss = 0.37531822\n",
      "Iteration 47, loss = 0.36942139\n",
      "Iteration 48, loss = 0.36318286\n",
      "Iteration 49, loss = 0.35658340\n",
      "Iteration 50, loss = 0.34960753\n",
      "Iteration 51, loss = 0.34224315\n",
      "Iteration 52, loss = 0.33448140\n",
      "Iteration 53, loss = 0.32631710\n",
      "Iteration 54, loss = 0.31775008\n",
      "Iteration 55, loss = 0.30878760\n",
      "Iteration 56, loss = 0.29944718\n",
      "Iteration 57, loss = 0.28975879\n",
      "Iteration 58, loss = 0.27976500\n",
      "Iteration 59, loss = 0.26951829\n",
      "Iteration 60, loss = 0.25907629\n",
      "Iteration 61, loss = 0.24849821\n",
      "Iteration 62, loss = 0.23784618\n",
      "Iteration 63, loss = 0.22719236\n",
      "Iteration 64, loss = 0.21662627\n",
      "Iteration 65, loss = 0.20625247\n",
      "Iteration 66, loss = 0.19617456\n",
      "Iteration 67, loss = 0.18647570\n",
      "Iteration 68, loss = 0.17721302\n",
      "Iteration 69, loss = 0.16843057\n",
      "Iteration 70, loss = 0.16017558\n",
      "Iteration 71, loss = 0.15249990\n",
      "Iteration 72, loss = 0.14544363\n",
      "Iteration 73, loss = 0.13901466\n",
      "Iteration 74, loss = 0.13317910\n",
      "Iteration 75, loss = 0.12786664\n",
      "Iteration 76, loss = 0.12298582\n",
      "Iteration 77, loss = 0.11844222\n",
      "Iteration 78, loss = 0.11415335\n",
      "Iteration 79, loss = 0.11005687\n",
      "Iteration 80, loss = 0.10611221\n",
      "Iteration 81, loss = 0.10229794\n",
      "Iteration 82, loss = 0.09860731\n",
      "Iteration 83, loss = 0.09504338\n",
      "Iteration 84, loss = 0.09161471\n",
      "Iteration 85, loss = 0.08833194\n",
      "Iteration 86, loss = 0.08520535\n",
      "Iteration 87, loss = 0.08224317\n",
      "Iteration 88, loss = 0.07945068\n",
      "Iteration 89, loss = 0.07682968\n",
      "Iteration 90, loss = 0.07437837\n",
      "Iteration 91, loss = 0.07209158\n",
      "Iteration 92, loss = 0.06996117\n",
      "Iteration 93, loss = 0.06797678\n",
      "Iteration 94, loss = 0.06612670\n",
      "Iteration 95, loss = 0.06439867\n",
      "Iteration 96, loss = 0.06278074\n",
      "Iteration 97, loss = 0.06126173\n",
      "Iteration 98, loss = 0.05983160\n",
      "Iteration 99, loss = 0.05848150\n",
      "Iteration 100, loss = 0.05720375\n",
      "Iteration 101, loss = 0.05599169\n",
      "Iteration 102, loss = 0.05483955\n",
      "Iteration 103, loss = 0.05374232\n",
      "Iteration 104, loss = 0.05269566\n",
      "Iteration 105, loss = 0.05169581\n",
      "Iteration 106, loss = 0.05073956\n",
      "Iteration 107, loss = 0.04982415\n",
      "Iteration 108, loss = 0.04894723\n",
      "Iteration 109, loss = 0.04810679\n",
      "Iteration 110, loss = 0.04730109\n",
      "Iteration 111, loss = 0.04652854\n",
      "Iteration 112, loss = 0.04578769\n",
      "Iteration 113, loss = 0.04507715\n",
      "Iteration 114, loss = 0.04439555\n",
      "Iteration 115, loss = 0.04374151\n",
      "Iteration 116, loss = 0.04311366\n",
      "Iteration 117, loss = 0.04251060\n",
      "Iteration 118, loss = 0.04193097\n",
      "Iteration 119, loss = 0.04137342\n",
      "Iteration 120, loss = 0.04083664\n",
      "Iteration 121, loss = 0.04031937\n",
      "Iteration 122, loss = 0.03982039\n",
      "Iteration 123, loss = 0.03933857\n",
      "Iteration 124, loss = 0.03887281\n",
      "Iteration 125, loss = 0.03842209\n",
      "Iteration 126, loss = 0.03798547\n",
      "Iteration 127, loss = 0.03756204\n",
      "Iteration 128, loss = 0.03715100\n",
      "Iteration 129, loss = 0.03675157\n",
      "Iteration 130, loss = 0.03636308\n",
      "Iteration 131, loss = 0.03598488\n",
      "Iteration 132, loss = 0.03561641\n",
      "Iteration 133, loss = 0.03525715\n",
      "Iteration 134, loss = 0.03490661\n",
      "Iteration 135, loss = 0.03456435\n",
      "Iteration 136, loss = 0.03422999\n",
      "Iteration 137, loss = 0.03390314\n",
      "Iteration 138, loss = 0.03358347\n",
      "Iteration 139, loss = 0.03327065\n",
      "Iteration 140, loss = 0.03296440\n",
      "Iteration 141, loss = 0.03266443\n",
      "Iteration 142, loss = 0.03237050\n",
      "Iteration 143, loss = 0.03208235\n",
      "Iteration 144, loss = 0.03179976\n",
      "Iteration 145, loss = 0.03152252\n",
      "Iteration 146, loss = 0.03125043\n",
      "Iteration 147, loss = 0.03098328\n",
      "Iteration 148, loss = 0.03072091\n",
      "Iteration 149, loss = 0.03046313\n",
      "Iteration 150, loss = 0.03020979\n",
      "Iteration 151, loss = 0.02996073\n",
      "Iteration 152, loss = 0.02971581\n",
      "Iteration 153, loss = 0.02947489\n",
      "Iteration 154, loss = 0.02923784\n",
      "Iteration 155, loss = 0.02900454\n",
      "Iteration 156, loss = 0.02877487\n",
      "Iteration 157, loss = 0.02854872\n",
      "Iteration 158, loss = 0.02832598\n",
      "Iteration 159, loss = 0.02810657\n",
      "Iteration 160, loss = 0.02789038\n",
      "Iteration 161, loss = 0.02767732\n",
      "Iteration 162, loss = 0.02746731\n",
      "Iteration 163, loss = 0.02726028\n",
      "Iteration 164, loss = 0.02705614\n",
      "Iteration 165, loss = 0.02685482\n",
      "Iteration 166, loss = 0.02665626\n",
      "Iteration 167, loss = 0.02646038\n",
      "Iteration 168, loss = 0.02626712\n",
      "Iteration 169, loss = 0.02607642\n",
      "Iteration 170, loss = 0.02588823\n",
      "Iteration 171, loss = 0.02570248\n",
      "Iteration 172, loss = 0.02551912\n",
      "Iteration 173, loss = 0.02533809\n",
      "Iteration 174, loss = 0.02515936\n",
      "Iteration 175, loss = 0.02498286\n",
      "Iteration 176, loss = 0.02480855\n",
      "Iteration 177, loss = 0.02463639\n",
      "Iteration 178, loss = 0.02446632\n",
      "Iteration 179, loss = 0.02429831\n",
      "Iteration 180, loss = 0.02413231\n",
      "Iteration 181, loss = 0.02396829\n",
      "Iteration 182, loss = 0.02380620\n",
      "Iteration 183, loss = 0.02364600\n",
      "Iteration 184, loss = 0.02348766\n",
      "Iteration 185, loss = 0.02333114\n",
      "Iteration 186, loss = 0.02317641\n",
      "Iteration 187, loss = 0.02302342\n",
      "Iteration 188, loss = 0.02287215\n",
      "Iteration 189, loss = 0.02272257\n",
      "Iteration 190, loss = 0.02257463\n",
      "Iteration 191, loss = 0.02242832\n",
      "Iteration 192, loss = 0.02228360\n",
      "Iteration 193, loss = 0.02214044\n",
      "Iteration 194, loss = 0.02199881\n",
      "Iteration 195, loss = 0.02185869\n",
      "Iteration 196, loss = 0.02172005\n",
      "Iteration 197, loss = 0.02158286\n",
      "Iteration 198, loss = 0.02144709\n",
      "Iteration 199, loss = 0.02131272\n",
      "Iteration 200, loss = 0.02117974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Air/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier=MLPClassifier(hidden_layer_sizes=(4,3),activation='logistic',solver='sgd',learning_rate_init=0.5,random_state=1,verbose=True)\n",
    "classifier.fit(values_train,target_train)\n",
    "target_pred=classifier.predict(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae0cf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11  0  0]\n",
      " [ 0 18  1]\n",
      " [ 0  3 12]]\n",
      "Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "cm=confusion_matrix(target_test,target_pred)\n",
    "print('Confusion Matrix:',cm,sep='\\n')\n",
    "acc=accuracy_score(target_test,target_pred)\n",
    "print('Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fad12c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09533946 -0.20981356  0.45574199  0.01925861]\n",
      " [-0.4427314  -0.81911754  0.43613687  0.33658788]\n",
      " [ 0.95316035  1.05475156 -1.18633571 -0.84970873]\n",
      " [ 1.78104406  1.99896864 -3.01009185 -2.41711311]]\n",
      "[[ 2.46229759  2.93318749 -2.32015007]\n",
      " [ 2.06322979  4.67105626 -1.74281517]\n",
      " [-4.51032489 -3.02835105  3.23701829]\n",
      " [-3.12949953 -0.64496047  2.68977946]]\n",
      "[[-2.40482808 -3.18898015  5.98217412]\n",
      " [-8.80950452  3.30021348  5.80611682]\n",
      " [ 3.77358509  1.77096533 -5.17781914]]\n"
     ]
    }
   ],
   "source": [
    "for i in classifier.coefs_:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
